
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Action-Free Reasoning for Policy Generalization</title>

    <meta name="description" content="Action-Free Reasoning for Policy Generalization">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://rt-h.github.io/img/RT-H/teaser_v3.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://rt-h.github.io/"/>
    <meta property="og:title" content="Action-Free Reasoning for Policy Generalization" />
    <meta property="og:description" content="Project page for RT-H: Action Hierarchies Using Language." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Action-Free Reasoning for Policy Generalization" />
    <meta name="twitter:description" content="Project page for Action-Free Reasoning for Policy Generalization." />
    <!-- <meta name="twitter:image" content="https://rt-h.github.io/img/random_img_frames.png" /> -->


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-52J0PM8XKV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-52J0PM8XKV');
</script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
        h1 {
            font-weight: bold;
        }
        h2 {
            font-weight: bold;
            text-align: center;
        }
        h4 {
            font-size: 23px;
        }
    </style>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <strong><font size="+6">Action-Free Reasoning for Policy Generalization</font></strong> </br> 
                <!--<small>
                    CoRL 2023
                </small>-->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center authors">
                <ul class="list-inline">
                <br>
                <li>
                    <a href="https://jadenvc.github.io">Jaden Clark</a><sup>1</sup>, 
		    <a href="https://suvirpmirchandani.com">Suvir Mirchandani</a><sup>1</sup>, 
                    <a href="https://dorsa.fyi/">Dorsa Sadigh</a><sup>1</sup>,
		    <a href="https://suneel.belkhale.com">Suneel Belkhale</a><sup>1</sup>
                </li>
                <br>
                </ul>
                <sup>1</sup>Stanford Universiy
            </div>
            <br>
        </div>

        <div class="row">
            <div class="col-md-12">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/pdf/2403.01823.pdf" target="_blank">
                        <image src="img/paper.png" height="60px">
                            <h4><strong>Paper to be released 2/6</strong></h4>
                        </a>
                    </li>
                    <!-- <li>
                        <a href="https://console.cloud.google.com/storage/browser/gdm-robovqa" target="_blank">
                        <image src="img/storage_icon.png" height="60px">
                            <h4><strong>Data</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/google-deepmind/robovqa/tree/main" target="_blank">
                        <image src="img/github_icon.png" height="60px">
                            <h4><strong>Code</strong></h4>
                        </a>
                    </li> -->
                    <!-- <li>
                        <a href="#videos">
                        <image src="img/youtube_icon.png" height="60px">
                            <h4><strong>Videos</strong></h4>
                        </a>
                    </li> -->
                </ul>
        </div>
        </div>
        <div class="row">
            <div class="col-md-12">


                <!-- <ul class="nav nav-pills nav-justified" style="text-align:center;"> -->
                <div style="width: 20%; float: left; text-align: center">
                    <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;padding:1px;">
                        <source src="videos/RT-H/sauce.mov" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <b>Put the sauce on the plate/b>
                </div>
                <div style="width: 20%; float: left; text-align: center">
                    <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;padding:1px;">
                        <source src="videos/RT-H/plushie.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <b>Pick up the plushie</b>
                </div>
                <div style="width: 20%; float: left; text-align: center">
                    <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;padding:1px;">
                        <source src="videos/RT-H/bottle.mov" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <b>Put the bottle on the book</b>
                </div>
                <div style="width: 20%; float: left; text-align: center">
                    <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;padding:1px;">
                        <source src="videos/RT-H/carrot.mov" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <b>Put the carrot on the rack</b>
                </div>
                <div style="width: 20%; float: left; text-align: center">
                    <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;padding:1px;">
                        <source src="videos/RT-H/sushi.mov" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <b>Put the sushi on the rack</b>
                </div>
                    <!-- <li>
                        <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;padding:1px;">
                            <source src="videos/RT-H/unstack_cups_trimmed_faster_cropped.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <b>Unstack cups</b>
                    </li>
                    <li>
                        <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;padding:1px;">
                            <source src="videos/RT-H/apple_in_jar_better_trimmed_faster_cropped.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <b>Apple in Jar</b>
                    </li> -->
                <!-- </div> -->

                <div style="width: 100%; text-align: center;">
                    <a href="https://rt-h.github.io/img/RT-H/two_step_green_slow.gif" target="_blank">
                        <figure>
                            <img src="img/RT-H/rad_front.png" class="img-responsive">
                        </figure>
                    </a>
                </div>
            </div>
        </div>

        <br>

        <div class="row" style="width: 80%; margin: 0 auto;"> 
            <div class="col-md-12">
                <h2>Abstract</h2>
                <p>
                    End-to-end imitation learning offers a promising approach for training robot policies. However, generalizing to new settings—such as unseen scenes, tasks, and object instances—remains a significant challenge. Although large-scale robot demonstration datasets have shown potential for inducing generalization, they are resource-intensive to scale. In contrast, human video data is abundant and diverse, presenting an attractive alternative. Yet, these human-video datasets lack action labels, complicating their use in imitation learning. Existing methods attempt to extract grounded action representations (e.g., hand poses), but resulting policies struggle to bridge the embodiment gap between human and robot actions. We propose an alternative approach: leveraging language-based reasoning from human videos - essential for guiding robot actions - to train generalizable robot policies. Building on recent advances in reasoning-based policy architectures, we introduce Reasoning through Action-free Data (RAD). RAD learns from both robot demonstration data (with reasoning and action labels) and action-free human video data (with only reasoning labels). The robot data teaches the model to map reasoning to low-level actions, while the action-free data enhances reasoning capabilities. Additionally, we release a new dataset of 3,377 human-hand demonstrations compatible with the Bridge V2 benchmark. This dataset includes chain-of-thought reasoning annotations and hand-tracking data to help facilitate future work on reasoning-driven robot learning. Our experiments demonstrate that RAD enables effective transfer across the embodiment gap, allowing robots to perform tasks seen only in action-free data. Furthermore, scaling up action-free reasoning data significantly improves policy performance and generalization to novel tasks. These results highlight the promise of reasoning-driven learning from action-free datasets for advancing generalizable robot control. 
                </p>
            </div>
        </div>

        <br>

        <h2>Video</h2>

        <p style="text-align:center; width: 80%; margin: 0 auto">
            <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;">
                <source src="videos/RT-H/ar_video-2.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </p>

        <br>
        <hr class="section-divider">

<!--         <div class="row">
            <div class="col-md-12">
                <h2>Grounding human video demonstrations through reasoning</h2>
                <h3> Language encodes the shared structure between similar tasks</h3>
                    <p>
                        <!-- Language enables us to break down complex concepts into digestible pieces.  -->
                        Human videos are plentiful, but it is unclear how to leverage this  data for training robot policies. We posit that learning <b>reasoning</b> (made up of high level scene and planning info) from human video data can effectively bridge the embodiment gap.
                        <!-- To bridge this divide between tasks and actions, our insight is to teach the robot the language of actions, describing low-level motions with more fine-grained phrases like "move arm forward" or "close gripper". 
                        Predicting these language motions as an intermediate step between high-level tasks and actions forces the policy to learn the shared structure of low-level motions across seemingly disparate tasks. 
                        Furthermore, a policy that is conditioned on language motions can easily be corrected during execution through human-specified language motions. 
                        This enables a new paradigm for flexible policies that can learn from human intervention in language. 
                        Our method RT-H builds an action hierarchy using language motions: it first learns to predict language motions, and conditioned on this along with the high-level task, it then predicts actions, using visual context at all stages. 
                        Experimentally we show that RT-H leverages this language-action hierarchy to learn policies that are more robust and flexible by effectively tapping into multi-task datasets. 
                        We show that these policies not only allow for responding to language interventions, but can also learn from such interventions and outperform methods that learn from teleoperated interventions. -->
                    </p>
                <br>
    
                <div style="width: 60%; margin: 0 auto">
                    <div style="width: 45%; float: left; text-align:center; margin: 0 auto">
                        <a href="https://rt-h.github.io/img/RT-H/pick_apple.gif" target="_blank">
                            <figure>
                                <img src="img/RT-H/pick_apple.gif" class="img-responsive">
                            </figure>
                        </a>
                        <figcaption>Pick the apple</figcaption>
                    </div>
                    <div style="width: 45%; float: right; text-align:center; margin: 0 auto">
                        <a href="https://rt-h.github.io/img/RT-H/knock.gif" target="_blank">
                            <figure>
                                <img src="img/RT-H/knock.gif" class="img-responsive">
                            </figure>
                        </a>
                        <figcaption>Knock the bottle over</figcaption>
                    </div>
                </div>

            </div>
        </div>

        <div class="row">
            <div class="col-md-12">
            <h3>Our insight is to teach the robot the <i>language of actions</i> </h3>
                <p>
                    To encourage data sharing, our insight is to describe low-level motions with more fine-grained phrases like "move arm forward" or "close gripper". 
                    We predict these language motions as an <b>intermediate step</b> between high-level tasks and actions, forcing the policy to learn the shared motion structure across tasks.
                    <!-- Experimentally we show that RT-H leverages this language-action hierarchy to learn policies that are more robust and flexible by effectively tapping into multi-task datasets. 
                    We show that these policies not only allow for responding to language interventions, but can also learn from such interventions and outperform methods that learn from teleoperated interventions. -->
                </p>

                <div style="width: 80%; text-align: center; margin: 0 auto">
                    <a href="https://rt-h.github.io/img/RT-H/shared_motions_slow.gif" target="_blank">
                        <figure>
                            <img src="img/RT-H/shared_motions_slow.gif" class="img-responsive">
                        </figure>
                    </a>
                </div>

            <h3>Language Motions enable easy <i>intervention</i> in language space</h3>
                <p>
                    Language motions enables a new paradigm for flexible policies that can learn from human intervention in language. We can provide corrective language motions to the policy at test time, and it will follow these motions to improve on the task. Then, we can learn from these interventions to improve the policy downstream.
                </p>

            <br>
                <a href="https://rt-h.github.io/img/RT-H/intervention_process_slow.gif" target="_blank">
                    <figure>
                        <img src="img/RT-H/intervention_process_slow.gif" class="img-responsive">
                    </figure>
                </a>
                <figcaption>RT-H learns the <i>language of actions</i>, representing low-level behaviors in language (<b>language motions</b>) as an intermediate layer in the policy. RT-H leverages a VLM co-trained on internet scale data to predict both the language motions from the task and the action from the language motions.</figcaption>
        
            </div>
        </div>
        
        <br>
        <hr class="section-divider"> -->

        <div class="row">
            <div class="col-md-12">
            <h2> Method </h2>
                <p>
                    RAD generates reasonings from both human video data and robot data 
                </p>
                <a href="https://rt-h.github.io/img/RT-H/method_slow.gif" target="_blank">
                    <figure>
                        <img src="img/RT-H/rad.png" class="img-responsive">
                    </figure>
                </a>
                <figcaption>RAD full pipeline overview</figcaption>

                <br>
                <figcaption> RAD generates reasonings on both human and robot data using a suite of pretrained models. Scene descriptors and object bounding boxes for both human and robot data are generated using Prismatic VLM and Grounding DINO. While SAM and proprioception can be used to generate movement primitives for robot data, \ACRO relies on HaMeR to track human hand data for primitive generation. For both data types, the scene descriptions, bounding boxes, and movement primitives (as well as actions for robot data) are synthesized by Gemini into reasoning data in natural language. These reasonings are tokenized and fed into a mixed dataset containing both human and robot data for co-finetuning.
                </figcaption>
                <br>
            </div>
        </div>

        <br>
        <hr class="section-divider">

        <div class="row">
            <div class="col-md-12">
                <h2> Experiments </h2>
                <p>We evaluate RAD across a variety of generalization tasks. These tasks comprise three main axes of generalization: <b>(1) Compositional Generalization </b>: In this axis, the objects, tasks, and scenes are all seen in pre-training data (Bridge V2 data), but not in those particular configurations. For example, pizza and salt both exist in Bridge V2, but salt is never placed on the pizza. <b>(2) New Object Generalization</b>: Unseen objects for known behaviors (e.g., "pick cup" to "pick plushie". <b>(3) Scene Generalization </b>: Generalizing to novel backgrounds and distractor objects for seen tasks; for example, picking up a known object with a pot in the background.</p>
                <!-- Training -->
                <br>
                <h3 class="bold-center">Cross-Embodiment Transfer</h3>
                <br>
		<p> First, we assess if \ACRO can learn accurate reasonings and robot actions on new tasks that are present only in human video demonstrations. </p>
                <a href="https://rt-h.github.io/img/RT-H/phase4_results_v7.png" target="_blank">
                    <figure>
                        <img src="img/RT-H/ce.png" class="img-responsive">
                        
                    </figure>
                </a>
                <figcaption>Fig. 3 - Results for models trained on new tasks represented only in human video data.
                </figcaption>
                <br>
                <p> <b>RAD outperforms baselines where human video data was trained on, but no new robot data was provided. RAD-A is RAD trained only on human video data for the given axis of generalization. ECoT-GT is finetuned on the same data as RAD, but only using human hand locations (and not the full reasoning data) </p>

            </div>
        </div>



	<div style="width: 95%; margin: 0 auto; text-align: center;">
	  <h3>
	    <b>ECoT</b> reasoning
	  </h3>
	  <a href="https://rt-h.github.io/img/RT-H/milk_bad.png" target="_blank">
	    <figure>
	      <img src="img/RT-H/milk_bad.png" class="img-responsive" 
	           style="display: block; margin: 0 auto;" 
	           alt="Milk carton - ECoT reasoning">
	    </figure>
	  </a>
	  <br>
	  <p>ECoT is unable to detect the milk carton and predicts the incorrect move.</p>
	</div>
	
	<!-- Second section: RAD reasoning (uncomment if you want to show it) -->
	<div style="width: 95%; margin: 40px auto 0; text-align: center;">
	  <h3>
	    <b>RAD</b> reasoning
	  </h3>
	  <a href="https://rt-h.github.io/img/RT-H/milk_good.png" target="_blank">
	    <figure>
	      <img src="img/RT-H/milk_good.png" class="img-responsive" 
	           style="display: block; margin: 0 auto;" 
	           alt="Milk carton - RAD reasoning">
	    </figure>
	  </a>
	  <br>
	  <p>RAD learns to identify the milk carton and chooses the correct move based solely on human video data.</p>
	</div>


	

	<div class="row">
	  <div class="col-md-4 text-center">
	    <video autoplay loop muted playsinline controls style="width: 100%;">
	      <source src="videos/RT-H/potato.mov" type="video/mp4">
	      Your browser does not support the video tag.
	    </video>
	    <p><i>Compositional: </i>Put the potato on the plate</p>
	  </div>
	  
	  <div class="col-md-4 text-center">
	    <video autoplay loop muted playsinline controls style="width: 100%;">
	      <source src="videos/RT-H/milk.mov" type="video/mp4">
	      Your browser does not support the video tag.
	    </video>
	    <p><i>New Scene: </i>Pick up the milk</p>
	  </div>
	
	  <div class="col-md-4 text-center">
	    <video autoplay loop muted playsinline controls style="width: 100%;">
	      <source src="videos/RT-H/cup.mov" type="video/mp4">
	      Your browser does not support the video tag.
	    </video>
	    <p><i>New Object: </i>Pick up the cup</p>
	  </div>
	</div>

<!--         <div class="row">
            <div class="col-md-12">
                <div>
                    <h3 style="text-align: center;">RAD Rollouts</h3>
                    <br>
                    <div style="width: 60%; margin: 0 auto">
                        <div style="width: 45%; float: left">
                            <p style="text-align:center;">
                                <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;">
                                    <source src="videos/RT-H/pull_napkin_cropped.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                </video>
                                Pull Napkin from Dispenser
                            </p>
                        </div>
                        <div style="width: 20%"></div>
                        <div style="width: 45%; float: right">
                            <p style="text-align:center;">
                                <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;">
                                    <source src="videos/RT-H/close_jar_cropped.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                </video>
                                Close Pistachio Jar
                            </p>
                        </div>
			 <div style="width: 20%"></div>
                        <div style="width: 45%; float: right">
                            <p style="text-align:center;">
                                <video id="bg-video" autoplay loop muted playsinline controls style="display: block; width: 100%;">
                                    <source src="videos/RT-H/close_jar_cropped.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                </video>
                                Close Pistachio Jar
                            </p>
                        </div>
                    </div>

                </div>

            </div>
        </div> -->

        <hr>
            
        <div class="row">
            <div class="col-md-12">
                <!--  -->
                <h3 class="bold-center">Generalization</h3>
                <a href="https://rt-h.github.io/img/RT-H/gen.png" target="_blank">
                    <figure>
                        <img src="img/RT-H/gen.png" class="img-responsive">
                    </figure>
                </a>
                <figcaption> Ultimately, training on large datasets of human video data should enable VLAs to generalize not only to human demonstrated tasks, but also to completely unseen scenarios. To explore if RAD enables training more general models, we evaluate our model against ECoT on 10 novel tasks (unseen in both human and robot data) comprising all three generalization axes.
                </figcaption>
                <br>
                <p>
                    RAD compared to ECoT for tasks contained in neither human or robot data. RAD shows improved performance across all three axes of generalization.
                </p>


		<div class="row">
		  <div class="col-md-4 text-center">
		    <video autoplay loop muted playsinline controls style="width: 100%;">
		      <source src="videos/RT-H/sauce.mov" type="video/mp4">
		      Your browser does not support the video tag.
		    </video>
		    <p><i>Compositional: </i>Put the green bottle on the plate</p>
		  </div>
		  
		  <div class="col-md-4 text-center">
		    <video autoplay loop muted playsinline controls style="width: 100%;">
		      <source src="videos/RT-H/carrot.mov" type="video/mp4">
		      Your browser does not support the video tag.
		    </video>
		    <p><i>New Scene: </i>Put the carrot on the rack</p>
		  </div>
		
		  <div class="col-md-4 text-center">
		    <video autoplay loop muted playsinline controls style="width: 100%;">
		      <source src="videos/RT-H/plushie.mp4" type="video/mp4">
		      Your browser does not support the video tag.
		    </video>
		    <p><i>New Object: </i>Pick up the plushie</p>
		  </div>
		</div>

    
            <div class="row">
                <div class="col-md-12">

                    <p>Similarly in this example, before intervention training, RT-H does not move its arm close enough to the bowl to grasp it. To correct this behavior, we can specify a correction online to tell the robot the move its arm forward more before it grasps. Training on these interventions, we find that RT-H-Intervene on the right is now better at moving the bowl away from the spout.</p>
                    
                </div>
            </div>

            <hr>

            <div class="row col-md-12">

                <!--  -->
                <h3 class="bold-center">Cross-Environment Transfer</h3>
                <p>To truly leverage large-scale video data, generalist robot policies must learn from demonstrations in diverse scenes. Thus, we first train \ACRO with human video data in unseen environments to see how well it can incorporate this data, and then we compare its performance to \ACRO trained on in-distribution human video data (i.e., same environment for both human video and robot evaluation).</p>
                <br>

		    

                <h4 class="bold-center">Data from outside target environment</h4>

		<div style="width: 80%; margin: 0 auto; display: flex; align-items: center; justify-content: space-between;">
		  <!-- Left side (Image) -->
		  <div style="width: 45%;">
		    <img src="img/RT-H/cross.png" alt="Cross" style="width: 100%;">
		  </div>
		
		  <!-- Right side (Video) -->
		  <div style="width: 45%;">
		    <video style="width: 100%;" autoplay loop muted playsinline controls>
		      <source src="videos/RT-H/tiger.mov" type="video/mp4">
		      Your browser does not support the video tag.
		    </video>
		  </div>
		</div>
  
                <p>
                    We collected data for four unseen tasks in a new tabletop setups (unseen in Bridge V2 data). Then, we evaluate models trained on this new enviroment data in the original Bridge Toy Sink environment. We find that models trained on this data outperform ECoT by 16% and ECoT-GT by 13%.
                </p>
                <br>

                <h4 class="bold-center">Scaling data collection</h4>

		<p> We assessed how RAD performance scales with increased data for the same tasks collected in-distribution (in the miniature Toy Sink setup) versus out-of-distribution (various real world kitchen and office environments). To do so, we collected 100 additional demos for the "pick up the tape" task in the Toy Sink setup. We also collected 250 out-of-domain demos for "pick up the tape" in novel environments such as real kitchens, countertops, and desks. Then, we trained RAD on two different data mixtures.</p>

		<div style="width: 80%; margin: 0 auto; display: flex; align-items: center; justify-content: space-between;">
		  <!-- Left side (Image) -->
		  <div style="width: 45%;">
		    <img src="img/RT-H/scale.png" alt="Cross" style="width: 100%;">
		  </div>
		
		  <!-- Right side (Video) -->
		  <div style="width: 45%;">
		    <video style="width: 100%;" autoplay loop muted playsinline controls>
		      <source src="videos/RT-H/tape.mov" type="video/mp4">
		      Your browser does not support the video tag.
		    </video>
		  </div>
		</div>
		

                <p>
                   We find that RAD models trained on both in-domain (+30% success) and out-of-domain data (+25% success) show improved performance over the original model. This indicates RAD scales well with diverse training data.
                </p>
                <br>



            </div>


        <hr class="section-divider">

         <div class="row">
            <div class="col-md-12">
                <h3>
                    Citation 
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{rth2024arxiv,
    title={Action-Free Reasoning for Policy Generalization},
    author={Jaden Clark and Suvir Mirchandani and Dorsa Sadigh and Suneel Belkhale},
    booktitle={https://arxiv.org/abs/2403.01823},
    year={2025}
}</textarea>
                </div>
                </div>
            </div>
             
        </div>


        <div class="row">
            <div class="col-md-12">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    <i>We thank Priya Sundaresan for and Juntao Ren for code for setting up HaMeR tracking. We thank Jonathan Yang, Satvik Sharma, Rahul Chand, and Priya Sundaresan for paper writing support. This work was supported by NSF #2006388 and #2125511, DARPA YFA Grant #W911NF2210214, ONR N00014-21-1-2298, Toyota Research Institute, and DARPA TIAMAT project.</i>
                    <br><br>
                The website template was borrowed from RT-H which was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
